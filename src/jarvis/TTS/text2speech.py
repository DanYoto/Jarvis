import requests
import soundfile as sf
import json
import numpy as np
import argparse
import re
from typing import List, Tuple, Generator
import cn2an
import threading
import queue
import time
import pyaudio

def normalize_numbers_chinese(text: str) -> str:
    """
    Normalize numbers in Chinese text to a consistent format.
    """
    def convert_number(match):
        number = match.group()
        try:
            # process decimal numbers
            if '.' in number:
                integer_part, decimal_part = number.split('.')
                integer_chinese = cn2an.an2cn(int(integer_part))
                decimal_chinese = 'ç‚¹' + ''.join([cn2an.an2cn(int(d)) for d in decimal_part])
                return integer_chinese + decimal_chinese
            
            # process integer numbers
            num = int(number)
            if num == 0:
                return 'é›¶'
            elif 1000 <= num <= 9999:
                # For years, use the digit-by-digit reading method
                if re.search(r'(19|20)\d{2}å¹´', match.string[max(0, match.start()-2):match.end()+1]):
                    return ''.join([cn2an.an2cn(int(d)) for d in number])
                else:
                    return cn2an.an2cn(num)
            else:
                return cn2an.an2cn(num)
                
        except:
            # If conversion fails, return digit-by-digit reading method
            return ''.join([cn2an.an2cn(int(d)) if d.isdigit() else d for d in number])

    # Match numbers (including decimals)
    text = re.sub(r'\d+\.?\d*', convert_number, text)
    return text


def normalize_special_symbols(text: str) -> str:
    symbol_map = {
        "%": "ç™¾åˆ†ä¹‹",
        "â€°": "åƒåˆ†ä¹‹",
        "â„ƒ": "æ‘„æ°åº¦",
        "Â°": "åº¦",
        "km": "å…¬é‡Œ",
        "m": "ç±³",
        "kg": "åƒå…‹",
        "g": "å…‹",
        "ml": "æ¯«å‡",
        "l": "å‡",
        "cm": "å˜ç±³",
        "mm": "æ¯«ç±³",
        "km/h": "å…¬é‡Œæ¯å°æ—¶",
        "m/s": "ç±³æ¯ç§’",
        "kmÂ²": "å¹³æ–¹å…¬é‡Œ",
        "mÂ²": "å¹³æ–¹ç±³",
        "ha": "å…¬é¡·",
        "t": "å¨",
        '&': 'å’Œ',
        '@': 'è‰¾ç‰¹',
        '#': 'äº•å·',
        '$': 'ç¾å…ƒ',
        'Â¥': 'äººæ°‘å¸',
        'â‚¬': 'æ¬§å…ƒ',
        'Â£': 'è‹±é•‘',
        '+': 'åŠ ',
        '-': 'å‡',
        'Ã—': 'ä¹˜ä»¥',
        'Ã·': 'é™¤ä»¥',
        '=': 'ç­‰äº',
        '<': 'å°äº',
        '>': 'å¤§äº',
        'â‰¤': 'å°äºç­‰äº',
        'â‰¥': 'å¤§äºç­‰äº',
        'â„‰': 'åæ°åº¦'
    }

    for symbol, replacement in symbol_map.items():
        text = text.replace(symbol, replacement)
    return text


def preprocess_text(text: str) -> str:
    """
    Process the input text to normalize numbers and special symbols.
    """
    # Normalize numbers
    text = normalize_numbers_chinese(text)
    
    # Normalize special symbols
    text = normalize_special_symbols(text)
    
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def split_text_intelligently(text: str, max_length: int = 256) -> List[str]:
    """
    Split text into chunks intelligently, grouping every two sentences together.
    """

    text = preprocess_text(text)

    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?]+)', text)
    
    # complete sentences is one sentence with punctuation
    complete_sentences = []
    for i in range(0, len(sentences), 2):
        if i + 1 < len(sentences):
            sentence_with_punct = sentences[i] + sentences[i + 1]
            if sentence_with_punct.strip():
                complete_sentences.append(sentence_with_punct)
        else:
            if sentences[i].strip():
                complete_sentences.append(sentences[i])
    
    if len(complete_sentences) <= 1:
        print("No sentence endings found, splitting by commas...")
        parts = re.split(r'([ï¼Œ,ã€ï¼›;]+)', text)
        complete_sentences = []
        for i in range(0, len(parts), 2):
            if i + 1 < len(parts):
                part_with_punct = parts[i] + parts[i + 1]
                if part_with_punct.strip():
                    complete_sentences.append(part_with_punct)
            else:
                if parts[i].strip():
                    complete_sentences.append(parts[i])

    print(f"Found {len(complete_sentences)} sentence/clause units")
    
    # combine two sentences into one chunk if they fit within max_length
    chunks = []
    for i in range(0, len(complete_sentences), 2):
        if i + 1 < len(complete_sentences):
            two_sentences = complete_sentences[i] + complete_sentences[i + 1]
        else:
            two_sentences = complete_sentences[i]
        
        if len(two_sentences) <= max_length:
            chunks.append(two_sentences.strip())
        else:
            if len(complete_sentences[i]) <= max_length:
                chunks.append(complete_sentences[i].strip())
            else:
                sub_chunks = split_long_sentence(complete_sentences[i], max_length)
                chunks.extend(sub_chunks)
            
            if i + 1 < len(complete_sentences):
                if len(complete_sentences[i + 1]) <= max_length:
                    chunks.append(complete_sentences[i + 1].strip())
                else:
                    sub_chunks = split_long_sentence(complete_sentences[i + 1], max_length)
                    chunks.extend(sub_chunks)
    
    for idx, chunk in enumerate(chunks):
        print(f"Chunk {idx + 1} ({len(chunk)} chars): {chunk[:50]}...")
    
    return chunks

def split_long_sentence(sentence: str, max_length: int) -> List[str]:
    """
    Split a long sentence by commas or other punctuation.
    """
    sub_parts = re.split(r'([ï¼Œ,ã€ï¼›;]+)', sentence)
    sub_chunks = []
    current_chunk = ""
    
    for part in sub_parts:
        if len(current_chunk) + len(part) <= max_length:
            current_chunk += part
        else:
            if current_chunk:
                sub_chunks.append(current_chunk.strip())
            current_chunk = part
    
    if current_chunk:
        sub_chunks.append(current_chunk.strip())
    
    return sub_chunks

def prepare_request(waveform, reference_text, target_text, sample_rate=16000):
    assert len(waveform.shape) == 1, "waveform should be 1D"
    lengths = np.array([[len(waveform)]], dtype=np.int32)
    
    samples = waveform.reshape(1, -1).astype(np.float32)

    data = {
        "inputs":[
            {
                "name": "reference_wav",
                "shape": samples.shape,
                "datatype": "FP32",
                "data": samples.tolist()
            },
            {
                "name": "reference_wav_len",
                "shape": lengths.shape,
                "datatype": "INT32",
                "data": lengths.tolist(),
            },
            {
                "name": "reference_text",
                "shape": [1, 1],
                "datatype": "BYTES",
                "data": [reference_text]
            },
            {
                "name": "target_text",
                "shape": [1, 1],
                "datatype": "BYTES",
                "data": [target_text]
            }
        ]
    }

    return data

def apply_crossfade(audio1: np.ndarray, audio2: np.ndarray, overlap_samples: int) -> np.ndarray:
    """
    æ”¹è¿›çš„äº¤å‰æ·¡å…¥æ·¡å‡ºï¼Œä½¿ç”¨æ›´è‡ªç„¶çš„è¿‡æ¸¡æ›²çº¿
    """
    if overlap_samples <= 0 or overlap_samples > len(audio1) or overlap_samples > len(audio2):
        return np.concatenate([audio1, audio2])
    
    fade_curve = np.linspace(0, 1, overlap_samples)
    # sigmoid-like
    fade_in = 0.5 * (1 + np.tanh(6 * (fade_curve - 0.5)))
    fade_out = 1 - fade_in
    
    audio1_fade = audio1.copy()
    audio2_fade = audio2.copy()
    
    audio1_end_rms = np.sqrt(np.mean(audio1[-overlap_samples:]**2))
    audio2_start_rms = np.sqrt(np.mean(audio2[:overlap_samples]**2))
    
    if audio1_end_rms > 0 and audio2_start_rms > 0:
        volume_ratio = audio1_end_rms / audio2_start_rms
        if volume_ratio > 2.0 or volume_ratio < 0.5:
            adjustment = np.sqrt(volume_ratio) if volume_ratio > 1 else 1/np.sqrt(1/volume_ratio)
            audio2_fade[:overlap_samples] *= min(adjustment, 1.5)
    
    audio1_fade[-overlap_samples:] *= fade_out
    audio2_fade[:overlap_samples] *= fade_in
    
    result = np.concatenate([
        audio1[:-overlap_samples],
        audio1_fade[-overlap_samples:] + audio2_fade[:overlap_samples],
        audio2[overlap_samples:]
    ])
    
    return result

class StreamingAudioPlayer:
    def __init__(self, sample_rate=16000, chunk_size=1024):
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.audio_queue = queue.Queue()
        self.playing = False
        self.generation_finished = False
        self.pa = pyaudio.PyAudio()
        self.stream = None
        self.chunks_played = 0
        
    def start_playback(self):
        """Start the audio playback stream"""
        self.stream = self.pa.open(
            format=pyaudio.paFloat32,
            channels=1,
            rate=self.sample_rate,
            output=True,
            frames_per_buffer=self.chunk_size
        )
        self.playing = True
        
        # Launch playback thread
        self.playback_thread = threading.Thread(target=self._playback_worker)
        self.playback_thread.daemon = True
        self.playback_thread.start()
        
    def add_audio_chunk(self, audio_data: np.ndarray, chunk_index: int = None):
        """Add audio data to the playback queue"""
        if chunk_index is not None:
            print(f"ğŸ“¥ Adding audio chunk {chunk_index + 1} to playback queue, length: {len(audio_data)} samples")
        self.audio_queue.put(audio_data)
        
    def finish_generation(self):
        """Mark generation as complete"""
        self.generation_finished = True
        print("ğŸ Audio generation complete, waiting for queue to empty...")
        
    def stop_playback(self):
        """Stop playback"""
        self.playing = False
        self.audio_queue.put(None)  # send stop signal
        
        if hasattr(self, 'playback_thread') and self.playback_thread:
            self.playback_thread.join(timeout=5.0)
            
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
            
        self.pa.terminate()
        
    def wait_for_completion(self):
        """Wait for all audio playback to complete"""
        while self.playing:
            if self.generation_finished and self.audio_queue.empty():
                if not self.stream.is_active():
                    self.playing = False
                    break
            time.sleep(0.05)

        if hasattr(self, 'playback_thread'):
            self.playback_thread.join(timeout=2.0)

        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
            self.pa.terminate()

    def _playback_worker(self):
        """Playback worker thread"""
        print("ğŸµ Starting audio stream playback...")

        while self.playing:
            try:
                audio_data = self.audio_queue.get(timeout=2.0)

                if audio_data is None:
                    print("ğŸ›‘ Stop signal received, exiting playback loop")
                    break

                self.chunks_played += 1
                print(f"â–¶ï¸ Playing chunk {self.chunks_played}, length: {len(audio_data)} samples")

                chunk_size = self.chunk_size
                for i in range(0, len(audio_data), chunk_size):
                    if not self.playing:
                        break
                    chunk = audio_data[i:i + chunk_size]
                    self.stream.write(chunk.astype(np.float32).tobytes())

                print(f"âœ… Chunk {self.chunks_played} playback finished")

            except queue.Empty:
                if self.generation_finished and self.audio_queue.empty():
                    print("â¹ï¸ Generation complete and queue empty, ending playback")
                    break
                continue
            except Exception as e:
                print(f"âŒ Playback error: {e}")
                break

        self.playing = False
        print(f"ğŸµ Playback finished, total chunks played: {self.chunks_played}")

        self.stream.stop_stream()
        self.stream.close()
        self.pa.terminate()

def generate_streaming_audio(
    server_url: str,
    model_name: str,
    reference_audio_path: str,
    reference_text: str,
    target_text: str,
    chunk_size: int = 200,
    overlap_duration: float = 0.1,
    sample_rate: int = 16000,
    player: StreamingAudioPlayer = None
) -> Generator[np.ndarray, None, None]:
    """
    Generate streaming audio, yielding audio segments as they are produced
    """
    # Load the reference audio
    reference_waveform, sr = sf.read(reference_audio_path)
    assert sr == sample_rate, f"Reference audio sample rate must be {sample_rate}"
    reference_samples = np.array(reference_waveform, dtype=np.float32)
    
    # Split the text
    chunks = split_text_intelligently(target_text, chunk_size)
    print(f"ğŸ“„ Text split into {len(chunks)} chunks")
    
    overlap_samples = int(overlap_duration * sample_rate)
    previous_audio = None

    for i, chunk in enumerate(chunks):
        print(f"ğŸ”„ Generating chunk {i+1}/{len(chunks)}: {chunk[:30]}...")
        
        try:
            # Prepare request
            data = prepare_request(reference_samples, reference_text, chunk)
            
            # Send request
            url = f"{server_url}/v2/models/{model_name}/infer"
            start_time = time.time()
            
            rsp = requests.post(
                url,
                headers={"Content-Type": "application/json"},
                json=data,
                verify=False,
                params={"request_id": str(i)}
            )
            rsp.raise_for_status()
            
            generation_time = time.time() - start_time
            print(f"âš¡ Chunk {i+1} generated in {generation_time:.2f}s")
            
            result = rsp.json()
            audio_data = result["outputs"][0]["data"]
            current_audio = np.array(audio_data, dtype=np.float32)
            
            # Apply crossfade except for the first chunk
            if previous_audio is not None and overlap_samples > 0:
                overlap_part = previous_audio[-overlap_samples:] if len(previous_audio) >= overlap_samples else previous_audio
                current_audio = apply_crossfade(overlap_part, current_audio, len(overlap_part))
                playback_audio = current_audio[len(overlap_part):]
            else:
                playback_audio = current_audio
            
            previous_audio = current_audio
            
            duration = len(playback_audio) / sample_rate
            print(f"âœ… Chunk {i+1} ready, duration: {duration:.2f}s, samples: {len(playback_audio)}")
            
            # Stream to player if available
            if player:
                player.add_audio_chunk(playback_audio, i)
            
            # Yield the audio segment
            yield playback_audio
            
        except Exception as e:
            print(f"âŒ Error processing chunk {i+1}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # Signal end of generation to player
    if player:
        player.finish_generation()
    

def synthesis_speech(server_url: str = "http://localhost:8000", 
         reference_text: str = "åƒç‡•çªå°±é€‰ç‡•ä¹‹å±‹ï¼Œæœ¬èŠ‚ç›®ç”±26å¹´ä¸“æ³¨é«˜å“è´¨ç‡•çªçš„ç‡•ä¹‹å±‹å† åæ’­å‡ºã€‚è±†å¥¶ç‰›å¥¶æ¢ç€å–ï¼Œè¥å…»æ›´å‡è¡¡ï¼Œæœ¬èŠ‚ç›®ç”±è±†æœ¬è±†è±†å¥¶ç‰¹çº¦æ’­å‡ºã€‚", 
         target_text: str = None, 
         reference_audio: str = "/Users/yutong.jiang2/Library/CloudStorage/OneDrive-IKEA/Desktop/Jarvis/src/jarvis/TTS/reference_audio/prompt_audio.wav", 
         model_name: str = "spark_tts",
         chunk_size: int = 200,
         overlap_duration: float = 0.1):

    server_url = server_url
    if not server_url.startswith(("http://", "https://")):
        server_url = f"http://{server_url}"
    
    
    print(f"ğŸ“ Target text length: {len(target_text)} characters")
    
    try:
        print("ğŸš€ Starting streaming audio generation and playback...")
        
        player = StreamingAudioPlayer(sample_rate=16000)
        player.start_playback()
        
        try:
            chunk_count = 0
            for audio_chunk in generate_streaming_audio(
                server_url=server_url,
                model_name=model_name,
                reference_audio_path=reference_audio,
                reference_text=reference_text,
                target_text=target_text,
                chunk_size=chunk_size,
                overlap_duration=overlap_duration,
                player=player
            ):
                chunk_count += 1
                print(f"ğŸ¼ Generator yielded chunk {chunk_count}")
            
            print(f"ğŸ All chunks generated, total: {chunk_count}")
            print("â³ Waiting for playback to finish...")
            player.wait_for_completion()
            print("âœ… Playback complete, exiting")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ Playback interrupted by user")
        finally:
            print("ğŸ›‘ Stopping player...")
            player.stop_playback()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Operation interrupted by user")
        return 1
    except Exception as e:
        print(f"âŒ Error: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    # reference_audio = "/Users/yutong.jiang2/Library/CloudStorage/OneDrive-IKEA/Desktop/Jarvis/src/jarvis/TTS/reference_audio/prompt_audio.wav"
    # reference_text = "åƒç‡•çªå°±é€‰ç‡•ä¹‹å±‹ï¼Œæœ¬èŠ‚ç›®ç”±26å¹´ä¸“æ³¨é«˜å“è´¨ç‡•çªçš„ç‡•ä¹‹å±‹å† åæ’­å‡ºã€‚è±†å¥¶ç‰›å¥¶æ¢ç€å–ï¼Œè¥å…»æ›´å‡è¡¡ï¼Œæœ¬èŠ‚ç›®ç”±è±†æœ¬è±†è±†å¥¶ç‰¹çº¦æ’­å‡ºã€‚"

    reference_audio = "/Users/yutong.jiang2/Library/CloudStorage/OneDrive-IKEA/Desktop/Jarvis/src/jarvis/TTS/reference_audio/yanglan_zh.wav"
    reference_text = "è¯­éŸ³åˆæˆæŠ€æœ¯å…¶å®æ—©å·²ç»æ‚„æ‚„èµ°è¿›äº†æˆ‘ä»¬çš„ç”Ÿæ´»ï¼Œä»æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹åˆ°æœ‰å£°è¯»ç‰©å†åˆ°ä¸ªæ€§åŒ–è¯­éŸ³å¤åˆ»ã€‚è¿™é¡¹æŠ€æœ¯æ­£åœ¨æ”¹å˜æˆ‘ä»¬è·å–ä¿¡æ¯ï¼Œä¸ä¸–ç•Œäº’åŠ¨çš„æ–¹å¼ï¼Œè€Œä¸”å®ƒçš„è¿›æ­¥é€Ÿåº¦è¿œè¶…æˆ‘ä»¬çš„æƒ³è±¡ã€‚"

    # target_text = "å¾®é£æ‹‚è¿‡æ¹–é¢ï¼Œæ€èµ·å±‚å±‚æ¶Ÿæ¼ªï¼Œæ˜ å‡ºæ–‘æ–“çš„å¤•é˜³ä½™æ™–ã€‚è¿œå¤„çš„é’å±±åœ¨è–„é›¾ä¸­è‹¥éšè‹¥ç°ï¼Œä¼¼ä¹åœ¨è¯‰è¯´ç€å¤è€çš„ä¼ è¯´ã€‚å²¸è¾¹çš„å‚æŸ³ä½å‚ææ¡ï¼Œä¼´éšç€é¸Ÿé¸£è½»è½»æ‘‡æ›³ï¼ŒçŠ¹å¦‚ä¸€æ›²æ‚ æ‰¬çš„å¤ç´ã€‚å‡ åªç™½é¹­æ—¶è€Œèµ·é£ï¼ŒæŒ¯ç¿…åˆ’è¿‡å¤©é™…ï¼Œåˆæ‚ ç„¶è½å›æ°´é¢ï¼Œç•™ä¸‹å‡ å£°æ¸…è„†çš„â€œå’•å’•â€ã€‚è¿™ä¸€åˆ»ï¼Œå¤©åœ°é™è°§ï¼Œå¿ƒå¢ƒæ¾„æ˜ï¼Œä»¿ä½›æ‰€æœ‰çš„çƒ¦æ¼éƒ½è¢«è¿™æ¸©æŸ”çš„æ™¯è‰²è½»è½»å¸¦èµ°ã€‚"
    target_text = "æ˜¥å¤©çš„é˜³å…‰é€è¿‡çª—æ£‚æ´’åœ¨æ¡Œæ¡ˆä¸Šï¼Œå¾®é£è½»æ‹‚è¿‡é™¢ä¸­çš„æ¡ƒèŠ±æ ‘ï¼ŒèŠ±ç“£çº·çº·æ‰¬æ‰¬åœ°é£˜è½åœ¨é’çŸ³æ¿ä¸Šã€‚è¿œå¤„ä¼ æ¥é¸Ÿå„¿æ¸…è„†çš„å•å•¾å£°ï¼Œä»¿ä½›åœ¨è¯‰è¯´ç€å­£èŠ‚æ›´æ›¿çš„å–œæ‚¦ã€‚è€äººååœ¨è—¤æ¤…ä¸Šï¼Œæ‰‹ä¸­æ§ç€ä¸€æ¯çƒ­èŒ¶ï¼ŒèŒ¶é¦™è¢…è¢…å‡èµ·ï¼Œä¸èŠ±é¦™äº¤èåœ¨ä¸€èµ·ï¼Œè¥é€ å‡ºä¸€ç‰‡å®é™ç¥¥å’Œçš„æ°›å›´ã€‚æ—¶å…‰åœ¨è¿™ä¸€åˆ»ä»¿ä½›æ”¾æ…¢äº†è„šæ­¥ï¼Œè®©äººä¸ç¦æ²‰é†‰åœ¨è¿™ä»½ç®€å•è€Œç¾å¥½çš„ç”Ÿæ´»ä¸­ã€‚"
    synthesis_speech(
        server_url="http://localhost:8000",
        reference_audio=reference_audio,
        reference_text=reference_text,
        target_text=target_text,
        model_name="spark_tts",
        chunk_size=40,
        overlap_duration=0.1
    )